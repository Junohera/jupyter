-------------------------------------------------------------
vectorization - 동시계산
cpu
-------------------------------------------------------------
array = numnpy format
-------------------------------------------------------------
numpy단점 gpu 미지원
--> tensor는 gpu로 지원해줌.
-------------------------------------------------------------
numpy format
	- sequence
	- immutable
	- homor
-------------------------------------------------------------
데이터저장하는 방식 세가지
python
c
portlan?
-------------------------------------------------------------
shape, dtype만 알고있으면 나머지 유추가능
-------------------------------------------------------------
shape이 같아야만 연산이가능
-------------------------------------------------------------
dtype
데이터 공간 ex) db 또는 c
-------------------------------------------------------------
ndim
차원
-------------------------------------------------------------
size
원소의 개수
-------------------------------------------------------------
itemsize = dtype/8
-------------------------------------------------------------
python의 list내부구조가
메모리가 비어있는 순서대로 저장 -> 이중구조 -> 인덱싱하는데 여러번 일을 수행함. + 데이터타입, 위치 체크

그러나, numpy Array는 한열로 저장하고, 데이터타입동일하기때문에 -> 인덱싱하는데 최소한의 일만 수행함.
-------------------------------------------------------------
strides (=pointer)
1칸에 4Byte, 1줄에 12Byte
-------------------------------------------------------------
loop연산속도
numpy vectorize > map function > list comprehonsion > plain forLoop
-------------------------------------------------------------

array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])

x[[True, True, False], [False, False, True, True]] # True인 행과 True인 열

-------------------------------------------------------------
axis
axis = 0 열마다
axis = 1 행마다
axis = None 전체

-------------------------------------------------------------
numpy 도움말 보기
1. np.info(np.sum) - 
2. np.lookfor('sum') - 문자열 포함된 모든 목록 찾아줌
-------------------------------------------------------------
?.shape
(x, y, z)
size, height, width
-------------------------------------------------------------
데이터 저장방식
대부분 C이나 간혹 포트란
c : 
1 2 3 4
5 6 7 8 
포트란 :
1 3 5 7
2 4 6 8
-------------------------------------------------------------
Multidimensional Arrays
ndarray
-------------------------------------------------------------
%whos ndarray # ndarray만 찾기 (numpy.ndarray)
-------------------------------------------------------------
uint16 (=unsigned )
-------------------------------------------------------------
영상, 이미지는 int16(for 256), 금융은 float64
-------------------------------------------------------------
ufunc (=universal Functions)

%time reduce(lambda x, y : x + y, range(100_000))
CPU times: user 12.7 ms, sys: 0 ns, total: 12.7 ms
Wall time: 12.2 ms
4999950000

%time np.add.reduce(np.arange(100_000))
CPU times: user 1.1 ms, sys: 0 ns, total: 1.1 ms
Wall time: 500 µs
4999950000

%time np.sum(np.arange(100_000))
CPU times: user 959 µs, sys: 311 µs, total: 1.27 ms
Wall time: 641 µs
4999950000

-------------------------------------------------------------
mean 평균
accumulate 누적합
-------------------------------------------------------------
차원끼리 연산하기위해서는 서로 모양이 같아야한다.
하지만 보정해주는 경우가 있는데 이를 활용한 기법이, broadcasting기법

# broadcasting

# 1.  
np.array([[1,2],[3,4]]) + 1

# 2. shape에 1만 있을경우. 크기 보정
np.array([[1,2],[3,4]]) + np.array([[4,5]])

# 3. 2와는 반대로
np.array([[1,2],[3,4]]) + np.array([[10],[10]])

# example
iris['petal_length'] + 10 # broadcasting
-------------------------------------------------------------
수동으로 맞추는방법 - 

1. reshape - 원본미접근, 모양기준, 2차로 변경되기도함.

# 수동으로 맞춰주는법
a = np.arange(10)
a = a.reshape(2, -1) # 모든 음수값은 가능하나 관례상 -1

2. resize - 원본접근, 갯수기준

3. ravel - view

4. flatten - copy
  
5. squeeze - 차원 축소

6. expand_dims - 차원 확대

7. newaxis - 
-------------------------------------------------------------
strides를 변경해도 가능
-------------------------------------------------------------
pickle
직렬화 기법
일반 텍스트가 아닌 데이터형을 유지한채 저장하는 방식
-------------------------------------------------------------
r_ : !callable
ix_ : callable
-------------------------------------------------------------
pandas 활용법 두가지
	- 1. EDA (Exploratory Data Analysis) 탐색적 데이터 분석
	- 2. 기계학습을 위한 전처리


-------------------------------------------------------------
numpyFormat
	- 1. pd.DataFrame(numpyFormat) # convert dataFrame
	- 2. pd.read_???
-------------------------------------------------------------
dataFrame 2열이상
	- 인덱스와 헤더가 존재하는 
	- .values하면 numpy포맷으로 변경
	- 2차원	
-------------------------------------------------------------
series 1열
	- .values하면 numpy포맷으로 변경
	- 1차원
-------------------------------------------------------------

열로 추출 - dictionary방식

b['사고년도']
-------------------------------------------------------------
.으로 접근.
dictionary방식 말고 (=data['column']) .방식도있음.
단, 특문이나 numpy의 키워드랑 겹치면안된다.
-------------------------------------------------------------
 (numpy syntax)
b.describe() # summary
b.describe().T #  열과 행 바꾸기 .T

-------------------------------------------------------------
데이터프레임 가져오면 하는것
1. .info()
2. describe() - include값이 기본적으로 숫자만 되어있으므로
-------------------------------------------------------------
loc는 이름으로
iloc는 순수 인덱스번호로
-------------------------------------------------------------
pancy indexing
	iris[iris.columns[:-1]] # pancy indexing
-------------------------------------------------------------
inplace - 그대로 적용을 시킬지의 여부(원본수정)
-------------------------------------------------------------
pandas에서는 index가 X축
-------------------------------------------------------------
# 컬럼추출방법 3가지

	# 1. series
	tips['tip'] 	

	# 2. dataFrame
	tips[['tip']] 

	# 3. 프로퍼티로접근
	tips.tip 
-------------------------------------------------------------
unique
	tips['day'].unique() # 서로다른 갯수
-------------------------------------------------------------
predicate함수

	boolean반환하는 함수
	bool이 왜 True|False 인데, 숫자 1과 0으로 취급되는가 ?
	상속받았는지 여부 확인하기.로 확인가능
	issubclass(bool, int)
-------------------------------------------------------------
시각화해서 보기 ( info, describe 보완 ) 

	mpg.boxplot()

-------------------------------------------------------------
상관계수 
data.corr()

# 색이 진하면 진할수록 음의 상관관계, 색이 옅을수록 양의 상관관계
sns.heatmap(mpg.corr()) # 색분포도로 데이터간 상관관계를 visualization화하여 보는 법
-------------------------------------------------------------
when import data

	1. head - 머리에서 데이터 추출

	2. tail - 바닥에서 데이터 추출

	3. sample - n개만큼 랜덤한 데이터 추출
-------------------------------------------------------------
graph그리기 
대상데이터.plot.원하는그래프형태()
-------------------------------------------------------------
기본적으로 세팅하는 라이브러리

	import numpy as np
	import matplotlib.pyplot as plt
	import pandas as pd
	import seaborn as sns
-------------------------------------------------------------
스타일을 바꾸게되면
	pandas가 plt에 의존하고있어서 pandas도 같이 바뀜
		plt.style.available # 사용가능한 스타일 목록조회
		plt.style.use('ggplot') # 사용하고싶은 스타일 세팅


	history
		from sadf import * # 편하긴하나 무겁.
		import matplotlib.pylab as plt # pylab이 가벼우나 공식홈페이지에서도 지양하는 방법
		import matplotlib.pyplot as plt # 결국 이걸로 사용
-------------------------------------------------------------
그래프수정하기 - State Machine기법 개념 이해해야함.

	plt.figure(figsize=(10, 5))
	plt.axes()
	plt.hist([1,2,1,2,3,4])
	plt.title('Title'); # 제목
	plt.xticks([1,2,3]); # x축 
	plt.yticks([1,3,5,7]); # y축
	plt.xlim([1,2]) # x범위 지정
	plt.ylim([1,2]) # y범위 지정
	plt.xlabel('xTicks Name') # x축 이름
	plt.ylabel('yTicks Name') # y축 이름

-------------------------------------------------------------

# 한글적용하기
	import matplotlib

	폰트리스트 확인
		fm = matplotlib.font_manager.FontManager()
		# fm.ttflist # 현재 컴퓨터에서 사용가능한 폰트리스트 확인
		# <Font 'Noto Sans CJK JP' (NotoSansCJK-Thin.ttc) normal normal 400 normal>

	폰트 적용
		plt.rcParams['font.family'] = 'Noto Sans CJK JP' # 한글 지원하는 폰트로 설정
	
-------------------------------------------------------------
%lsmagic # jupyter에서 지원하는 매직펑션 확인

%matplotlib notebook # 팝업처럼나옴, 물론 현재 os설정에 따라 달라지기도함.
%matplotlib inline # 기본값(=노잼)
-------------------------------------------------------------
pd.read_csv
	header : 헤더 여부

	sep : 구분자
-------------------------------------------------------------
pandas 컬럼 지우기

drop

# data.drop(1, axis=1, inplace=True)
-------------------------------------------------------------
encoding Error 대응

	 Error tokenizing data. C error: Expected 1 fields in line 12, saw 3

	 engine='python' ||
	 sep='|' ||
	 encoding= isEng & 'latin1' || isKo & 'cp949'
-------------------------------------------------------------
pandas 필수 세가지

	aggregation

		groupby
			1. split
				특정 컬럼으로 쪼갬
				
			2. apply
				.mean
				.count
				.max
				.min

			3. combine
				2를 다시 합쳐줌
		pivot
			합쳐서 표현하는게 아니라 단건단건 점찍어서 표현?
		crosstab
-------------------------------------------------------------
!tidy(=wide) -> melt() -> tidy

id_vars : 변경하지 않을 값
-------------------------------------------------------------
vars : instance가 가진 값
-------------------------------------------------------------
데이터 분석 기본 프로세스

	1. import data
	2. isTidy(data) ? 
-------------------------------------------------------------
컬럼명 변경
-------------------------------------------------------------
인덱스만 추출
.index
컬럼만 추출
.column
-------------------------------------------------------------
NaN값 지우기 - missingData

dropna() 
-------------------------------------------------------------
합치기
	stack
	concat
		pd.concat([a,c], axis=1) # a와 c를 concatnate
-------------------------------------------------------------
default setting

import folium
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

plt.style.use('ggplot')
plt.rcParams['font.family'] = 'Noto Sans CJK JP'

-------------------------------------------------------------
pandas.wide_to_long(data)
melt나오기전에 유일한 방법인데, 쓰기 어려움..., 형태를 완벽하게 맞춰야하는 번거로움이 존재.
범용적으로는 melt를 이용하지만, 알고는 있으셈! 
-------------------------------------------------------------
reset_index
	인덱스 번호 변경
	
-------------------------------------------------------------
jupyter notebook theme
pip install jupyterthemes

available list	
	jt - l

set theme
	jt -t chesterish -T -T -cellw 98%
-------------------------------------------------------------
지도 벡터 표현방식 두가지
in Web
topo_json geo_json
-------------------------------------------------------------
from pprint import pprint
pretty 
-------------------------------------------------------------
import time
	시각

현재시각
timestamp :from 1970-01-01

현재로컬시각
localTime = time.localtime()

	# 달로 접근`하기
	localTime.tm_mon # dic
	localTime[1]` # list
-------------------------------------------------------------
import calendar
	달력 찍어주는애
-------------------------------------------------------------
import datetime
	

	import datetime

	x = datetime.date(2019, 11, 27)

	x.year # o
	# x['year']  x
	# x[0] x
-------------------------------------------------------------
인덱스 2개

new_dat스.set_index(['Date', 'forecast']) # 동시에 2개 인덱스

-------------------------------------------------------------
stack (melt와 비슷, melt가 좀더 간편)
	컬럼에 있는 값을 인덱스로 변경
-------------------------------------------------------------
stack과 unstack을 이용해서 원하는대로 구조변경가능

-------------------------------------------------------------
duplicated
	중복체크
	반대는 dropduplicated?
-------------------------------------------------------------
귀여운 그래프스타일
1 plt.xkcd()
2 plt.style.use('ggplot')

plt.rcdefaults() # 초기화

한번만 귀여운스타일
with plt.xkcd():
    tips[['tip']].plot.bar()
-------------------------------------------------------------
1. read_csv
2. tidy_data
	melt
	set_index, stack
	wide_to_long
3. info
	data-type
	head
	miss

4.
5.d

-------------------------------------------------------------
df['column'].value_counts()
	해당 컬럼의 값 갯수 # df(DataFrame)의 column이란 컬럼의 값 갯수
-------------------------------------------------------------
DataFrame에서 
	map
		- only series
		t['petal length (cm)'] = t['petal length (cm)'].map(lambda x:x+1) # map series에서만 사용 가능
	apply
		- series & DataFrame
		- axis, reduce도 포함되어있음.
		t['petal length (cm)'] = t['petal length (cm)'].apply(lambda x:x+1) # apply series&DataFrame 사용 가능
	applymap
		- only DataFrame
-------------------------------------------------------------
데이터범위 지정하고싶을때 사용
	a = np.array([1,2,3,4,5,6,7])
	np.clip(a, 2,5)  # 2 if 2>x elif 5<x 5, 2보다 작으면 2 5보다 크면 5(=fix range(2to5))
	# array([2, 2, 3, 4, 5, 5, 5])
-------------------------------------------------------------
비정형 데이터에서 가장 성능이 좋음.
굳이 인공지능이 아니더라도 똑같이 꾸릴 수있으나,,
가격과 성능을 비교하여 인공지능을 대입할지 

Data로부터 Specific 문제 해결을 위한 최적의 모델 만들기.
Data(비정형데이터 ex: 자연어, 이미지, 영상...)
Specific(매우 협소한 특정 기능, 특정분야 한가지만 가능. 여러가지 동시에 하지못함. 기존에 존재하던 기술을 기반으로 ... 한두가지정도)

기계학습 : 정형데이터가 성능이 좋음.
딥러닝 : 정형데이터도 활용가능하긴함... 그러나...비정형데이터가 성능이 좋음
-------------------------------------------------------------
인공지능의 시작은 
데이터 ==> pandas

가용한 데이터인지 알수 있으려면, 
	1. EDA(비쥬얼라이제이션을 통해 데이터에 대한 빠른 분석) Exploratory Data Analysis
	2. 데이터 전처리(정제) 
그리고 쉽게 정제하기위함. ==> pandas

기계학습의 성능: 알고리즘 < 데이터
-------------------------------------------------------------
인공지능 : 데이터 분석 = 기존 IT : 요구사항 분석

-------------------------------------------------------------
seaborn = 간편하게 그래프출력 + 연습용 데이터
import seaborn as sns # 통계적 그래프를 간단하게 그려줌.
-------------------------------------------------------------
from sklearn.datasets import 
	1. load 작은데이터
	2. fetch 웹데이터
	3. make 랜덤데이터
-------------------------------------------------------------
컬럼 feature || dimension || x값

-------------------------------------------------------------
리그레이션
	실수면 data.info로 확인
클래스피케이션?
-------------------------------------------------------------
기계학습은 반드시 숫자여야만함. (= 인코딩)
통계에 의존성이 큼. (성능과 연관 깊음.)
-------------------------------------------------------------
sns.pairplot = 그려놓고 시작

	sns.pairplot(iris.iloc[:, :-1]) # data는 pandas여야함! 4 x 4 = 16, 16개 출력
		# 그래프 그리기 1. 
	sns.pairplot(iris, vars=iris.columns[:-1], hue='target') # data는 pandas여야함! 4 x 4 = 16, 16개 출력
		# 그래프 그리기 2. hue는 색깔, vars는 사용할 데이터 (= hue에는 'target'은 써야하지만, 그래프에 출력시키면 안되므로..)

-------------------------------------------------------------
ready 4 edu
	x_train, x_test, y_train, y_test = train_test_split(iris.iloc[:,:-1], iris.target)  # x = 0~4줄, y = 1줄, target, label, class값

-------------------------------------------------------------
Features(attributes, dimensions)


classifier	- 영역
	(개,고양이) , (붓꽃종류) 구분
	supervised(분류)
	unsupervised(군집화)

regression 
	나이 숫자 
	선형 회귀(線型回歸, 영어: linear regression)는 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법

쉬운건 영역이 더쉬움
-------------------------------------------------------------
관점별 중요시하는 것
	
	통계 - 해석
	인공지능 - 성능
-------------------------------------------------------------
no free lunch

머신러닝에서는 절대 완벽한 알고리즘은 존재 할 수 없으니 꾸준히 노력하고 여러 분야에 대해 이론을 만들어야 한다는 얘기

why? reason best (=no free lunch)

 ==> 해봐야암 just try

-------------------------------------------------------------
차원의저주


	데이터의 차원이 증가할 수록 (모수가 늘어나므로 ===> 데이터의 개수가 기하급수적으로 늘어나는 문제)

	저차원에서는 충분했던 데이터 양도 고차원이 되면 공간(학습 모델)을 설명하기에 부족해질 수 있음.

	데이터가 작

극복하는법
	
	쓰레기데이터를 수정하거나 제거

	info
		개수가 몇개없으면 모델을 설명하기에 데이터가 부족
-------------------------------------------------------------
data > algo
-------------------------------------------------------------
기계학습은 한계가 있음.
그러나 딥러닝은 데이터와 계속 비례
-------------------------------------------------------------
정형데이터의 경우 성능이
딥러닝 < 기계학습
인 경우도 많음.
-------------------------------------------------------------
놀이터
	http://playground.tensorflow.org
	hidden layer & 알고리즘 & 노드 세팅
-------------------------------------------------------------
블랙박스
	end to end로 결과로 해결된 것을 확인했으나, 과정과정마다의 원리를 아직까지 사람을 이해하지 못한 것.
	설명이 필요한 분야에서는 해당 이슈때문에 사용하지못함 (ex: 의학, ...)
	설명 필요없고 성능만 나와도 사용하는 경향으로 기울어지고있음...
-------------------------------------------------------------
기계학습
	양적데이터 > 질적데이터
-------------------------------------------------------------
쓸모없는 데이터는 필요없지만,

쓸모없는 데이터라고해서 그냥 버릴까? 
no ->
-------------------------------------------------------------
Data wrangling || Data muning

	복잡하고 지저분한 상태의 데이터를 간단한 분석과 접근을 위해 통합하는 과정
-------------------------------------------------------------
모델은 이미 많이 만들어져있다.
승패는 데이터 전처리를 어떻게 하느냐에 따라...
-------------------------------------------------------------
데이터

	정형
		관계형 DB
		컬럼마다 의미가 있음.
		컬럼위치를 변경해도 문제가되지않음.

	반정형

	비정형
		컬럼위치를 변경하면 문제가됨.(ex:image)
		컬럼에 의미가 없음.
		순서가 의미
-------------------------------------------------------------
기계학습 데이터세팅

	정형 -> series

	비정형 -> 1차원
		성능에서 딥러닝알고리즘이 최적
		음성, 자연어, 영상 모두 비정형이므로 딥러닝에서 성능이 가장 잘나옴.
-------------------------------------------------------------
image 전처리하기
	0~255 => 0~1
-------------------------------------------------------------
문자를 숫자로 바꾸는 방법 2가지

	1. label encoding

		- pandas
			# lambda
			tips.sex.map(lambda x: 1 if x == 'Female' else 2)
			# dictionary
			tips.sex.map({'Female':0, 'Male':1})

			# 요일을 숫자로바꾸기

			# dictionary
			dayEncoding = {'Sat':0,'Sun':1,'Thur':2,'Fri':3}
			tips.day.map(dayEncoding)
			# lambda
			#tips.day.map(lambda x: 1 if x == 'Female' else 2)

			pass

		- sklearn

			from sklearn.preprocessing import ???

			# le.fit(mpg.origin)
			# le.transform(mpg.origin)

			# 한방
			mpg.origin = le.fit_transform(mpg.origin)
			# mpg.origin.map({'usa':0, 'europe':1, 'japan': 2}) # pandas로 하게되면 되돌릴수 없으나, 장단점이있음.		


	2. one hot encoding
		
		- pandas
			
			pd.get_dummies(mpg.origin)

		- sklean

			from sklearn.preprocessing import OneHotEncoder

			ohe = OneHotEncoder()

			# ohe.fit_transform(mpg[['origin']]) 해서

			#<398x3 sparse matrix of type '<class 'numpy.float64'>'
				#with 398 stored elements in Compressed Sparse Row format>
					#가 나오면 toarray        
					
			ohe.fit_transform(mpg[['origin']]).toarray()
			# ohe.inverse_transform([[0,0,1]])
-------------------------------------------------------------
기계 학습 준비물 만들기

data.target_names
a = pd.DataFrame(data.data, columns=data.feature_names) # DataFrame Instance
b = pd.DataFrame(data.target, columns=['target'])
iris = pd.concat([a,b],axis=1) # data ready done
-------------------------------------------------------------
hold out - train_test_split
	가장 좋은 알고리즘을 찾기위한 
	데이터의 공정성을 최대화.
		no free lunch(데이터에 따라 성능차이가 있기때문에)


crossValidation
		데이터가 너무 적을경우,, 하나만 틀려도 성능차이에 큰 요인으로 다가오므로,,,
		모든데이터(트레인, 테스트데이터)가 평균적으로 들어가므로...
		데이터를 나눌때 고려할 필요없이 편리하게 사용 가능
-------------------------------------------------------------
train_test_split

	random_state

	stratify ()
		클래스피케이션일때만가능
		y값(target) 기준으로 비율을 맞춰서 잘라주는 옵션
			데이터가 적을때 사용하면 좋음.
				데이터가 많으면 큰수의법칙에 의해 굳이 필요없는 옵션이지만, 
				그렇지 않을 경우에는 해당 옵션을사용하여 큰수의법칙 부정적인 영향을 방어

	n_jobs
		-1 : 프로세서 풀가동

	
-------------------------------------------------------------
다중 공선성
-------------------------------------------------------------
큰 수의 법칙

	작은 데이터에서는 균일하지 못하게 나올 확률이 높지만,
	데이터의 양이 많으면 많을수록 평균적인 분포와 가까워진다는 법칙
	ex) 주사위

-------------------------------------------------------------
2종 오류
	2ne

-------------------------------------------------------------
cross_val_score
	데이터가 적을때 반드시 사용할 것.
	같은 데이터를 여러 방면으로 시도하기때문에 더 많은 데이터를 사용하는 것과 비슷한 효과를 가지게된다.(=데이터확보)
	+ hyper parameter 찾을때 사용 (hyper parameter : 사람이 찾아줘야하는 데이터)

	- 이용못하는 경우 : 최종결과물에만 크로스벨리데이션(crossValidation)기법을 사용하지않음.

	from sklearn.model_selection import cross_val_score

	from sklearn.neighbors import KNeighborsClassifier
	from sklearn.ensemble import RandomForestClassifier

	cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:, -1], cv = 10)


		estimator
			알고리즘을 인스턴스
		
-------------------------------------------------------------
sklearn에서 기본적으로 숫자데이터만 처리가가능하지만,

Y값을 문자로 썼을때는 숫자로 바꿔줌...
-------------------------------------------------------------
pip install pandas-profiling

import pandas_profiling as pp

ProfileReport
-------------------------------------------------------------
모델이 복잡할수록 성능이 좋음.
 + 차원이 증가 === 데이터가 많이필요.
-------------------------------------------------------------
Occam's Razor 오캄의 면도날

	동가홍상
		비슷한 성능이면 더심플한 것을 선택
			=== 간단한 모델을 선택
				=== 데이터양 더 적게, 
-------------------------------------------------------------
편향과 편차를 잘 조절해야함.

편향과 편차의 관계

bias(편향)/variance(편차)

	overfitting (기계학습에서 )
		- 지나치게 값에 치우쳐짐. (학습데이터에서는 좋지만, 실제 예측할때는 성능이 좋지않음.)
		- 편차가 큰 모델
		- 모델이 복잡해질수록 오버피팅 확률높아짐

	underfitting
		- 지나치게 몰려져있음
		- 편향이 큰 모델
		- 일어나는 확률이 높음
	
-------------------------------------------------------------
모델의 고려사항

	성능
	편차
-------------------------------------------------------------
cv를 했을 경우, 데이터 관리

from sklearn.model_selection import KFold # 데이터관리할때 편리함. 어떤 데이터를 사용했는지.

kf = KFold(7, shuffle=True)
cross_val_score(KNeighborsClassifier(), wine.iloc[:,:-1], wine.target, cv = kf)
for trainData, testData in kf.split(wine.iloc[:,:-1], wine.target):
    print(trainData) # 트레인데이터
    print(testData) # 테스트데이터
-------------------------------------------------------------
train_score train_data로 테스트한 데이터 
test_score test_data로 테스트한 데이터?

-------------------------------------------------------------
# cv에 사용할수 있는 애들.

from sklearn.model_selection import StratifiedKFold # class비율까지 고려한 KFold (stratify를 고려) === 더 정확한 테스트 가능 == 
from sklearn.model_selection import ShuffleSplit # 셔플
from sklearn.model_selection import StratifiedShuffleSplit # class비율 고려하고 셔플
-------------------------------------------------------------
표준편차 기법 (표준화)

	편차를 줄여서 영향을작게
	ex) mean() 한값이 크게나오는 경우 표준편차를 사용

-------------------------------------------------------------
Perceptron 딥러닝과 관련된 알고리즘
-------------------------------------------------------------
in sklearn

	learningCurve
		러닝커브가 더 중요함.

	validationCurve
		gridSearch 방식으로 대체가능(타 프레임워크에서는 없는 경우도 있으므로 validationCurve도 사용할 수 있어야함.)
-------------------------------------------------------------
	statistical learningCurve
		설명이 가능해야함 => 간단 => 차원축소

	machine learningCurve

		
-------------------------------------------------------------

feature_selection ( 성능 문제 때문에 )
-------------------------------------------------------------
	chi - squared
	카이 스퀘어?

	SelectKBest 

		# 오버피팅 방어
		# 가장 좋은 n개 선택하는 법 - 10개 데이터중에 가장 중요한 n개만 선택
		from sklearn.feature_selection import chi2, SelectKBest 

	

-------------------------------------------------------------
sklearn

	datasets
		3
	
	model_selection

	preprocessing

	feature_selection

	metrics

-------------------------------------------------------------
TP : true - true
TN : false - false

precision C = 
-------------------------------------------------------------
LogisticRegression
	타 프레임워크에서는 3개이상 구분을하지못함. linear모델이기 때문에 두가지만 구분가능.
	하지만 sklearn에서는 내부적으로 처리해줌.
-------------------------------------------------------------
레이어가 많으면 성능이 좋아짐.
	대신 그리고 미분을 계속하다가 0이되버리는 순간에 업데이트가 안될수도있고,
	오버피팅되는 문제와 학습안되는 문제를 해결하지못했었음.
	하지만 해결한게 딥러닝


	그럼에도 복잡하면 복잡할수록 오버피팅이 생김.
	
		
-------------------------------------------------------------

집단지성

	VotingClassifier

	제일 좋은 모델찾기위해서 서로 대결 시키는건데, 리소스가 엄청나게 많이들어감.

	합치면 강해지는 경우, 
	서로 싸워서 더 높은 결과를 반환
	=> 예측모델만들 때, 보통 마무리단계에서 앙상블을 사용함.
	current 60 , need 80 ,if use ensemble, possible over 80
	weights 가중치
	
	BaggingClassifier
		오버피팅이 잘나는 모델에 대해서 
		학습데이터에 너무 치우쳐서 학습된 모델일때,

	boosting
		오버피팅 잘나지만, 성능 좋음. but 시간이오래걸림
		--> 오버피팅 잘나지만 성능을 우선 올리고, 오버피팅을 보완하는 식으로 가는것이 현 메타?

		임계치를 정해두고, 임계치에 올라올때까지 교육시키고, 넘어가면 꺼내옴.

		AdaBoostClassifier
			
			base_estimator 교육시킬 모델

		gradient_boosting

		xdboosting?
			현재 가장 많으쓰인다고함.

		stackingClassifier
			학습된 결과가 모델로 됨.
			속도는 진짜 느리지만, 성능에서 최고.

-------------------------------------------------------------
mlxtend
	
-------------------------------------------------------------
deep learnigng

	numpy 
-------------------------------------------------------------
# 첫번째 이미지 추출하기
# X_train.shape

# X_train[0, :, :]
# X_train[0,...]
X_train[0]
-------------------------------------------------------------
학습시킬때는 반드시
1차원으로 만들어주어야함.
예를들어 이미지 2차원짜리도 1차원으로 쭉 늘여야만 학습이 가능한 데이터가 된다.
ex) numpy포맷일 경우, reshape이나 flatten으로 ...
-------------------------------------------------------------
레이어가 간단할경우.
직선이고, 
그런 레이어가 중첩되면 될수록,
곡선을 표현할수있게된다.
-------------------------------------------------------------
AND gate

	w1x1 + w2x2 + b

	x1 x2
	0  0 => 0 : b < 0
	0  1 => 0 : w2 + b < 0
	1  0 => 0 : w1 + b < 0
	1  1 => 1 : w1 + w2 + b < 0

OR gate

	x1 x2
	0  0 => 0 : b < 0 
	0  1 => 1 : w2 + b > 0
	1  0 => 1 : w1 + b > 0
	1  1 => 1 : w1 + w2 + b > 0

	b = -0.1
	w2 > 0.1 -> w2 = 0.2
	w1 > 0.1 -> w1 = 0.2

	0.2 + 0.2 + -0.1 > 0 
	

	
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------
-------------------------------------------------------------