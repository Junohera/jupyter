-------------------------------------------------------------
vectorization - 동시계산
cpu
-------------------------------------------------------------
array = numnpy format
-------------------------------------------------------------
numpy단점 gpu 미지원
--> tensor는 gpu로 지원해줌.
-------------------------------------------------------------
numpy format
	- sequence
	- immutable
	- homor
-------------------------------------------------------------
데이터저장하는 방식 세가지
python
c
portlan?
-------------------------------------------------------------
shape, dtype만 알고있으면 나머지 유추가능
-------------------------------------------------------------
shape이 같아야만 연산이가능
-------------------------------------------------------------
dtype
데이터 공간 ex) db 또는 c
-------------------------------------------------------------
ndim
차원
-------------------------------------------------------------
size
원소의 개수
-------------------------------------------------------------
itemsize = dtype/8
-------------------------------------------------------------
python의 list내부구조가
메모리가 비어있는 순서대로 저장 -> 이중구조 -> 인덱싱하는데 여러번 일을 수행함. + 데이터타입, 위치 체크

그러나, numpy Array는 한열로 저장하고, 데이터타입동일하기때문에 -> 인덱싱하는데 최소한의 일만 수행함.
-------------------------------------------------------------
strides (=pointer)
1칸에 4Byte, 1줄에 12Byte
-------------------------------------------------------------
loop연산속도
numpy vectorize > map function > list comprehonsion > plain forLoop
-------------------------------------------------------------

array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])

x[[True, True, False], [False, False, True, True]] # True인 행과 True인 열

-------------------------------------------------------------
axis
axis = 0 열마다
axis = 1 행마다
axis = None 전체

-------------------------------------------------------------
numpy 도움말 보기
1. np.info(np.sum) - 
2. np.lookfor('sum') - 문자열 포함된 모든 목록 찾아줌
-------------------------------------------------------------
?.shape
(x, y, z)
size, height, width
-------------------------------------------------------------
데이터 저장방식
대부분 C이나 간혹 포트란
c : 
1 2 3 4
5 6 7 8 
포트란 :
1 3 5 7
2 4 6 8
-------------------------------------------------------------
Multidimensional Arrays
ndarray
-------------------------------------------------------------
%whos ndarray # ndarray만 찾기 (numpy.ndarray)
-------------------------------------------------------------
uint16 (=unsigned )
-------------------------------------------------------------
영상, 이미지는 int16(for 256), 금융은 float64
-------------------------------------------------------------
ufunc (=universal Functions)

%time reduce(lambda x, y : x + y, range(100_000))
CPU times: user 12.7 ms, sys: 0 ns, total: 12.7 ms
Wall time: 12.2 ms
4999950000

%time np.add.reduce(np.arange(100_000))
CPU times: user 1.1 ms, sys: 0 ns, total: 1.1 ms
Wall time: 500 µs
4999950000

%time np.sum(np.arange(100_000))
CPU times: user 959 µs, sys: 311 µs, total: 1.27 ms
Wall time: 641 µs
4999950000

-------------------------------------------------------------
mean 평균
accumulate 누적합
-------------------------------------------------------------
차원끼리 연산하기위해서는 서로 모양이 같아야한다.
하지만 보정해주는 경우가 있는데 이를 활용한 기법이, broadcasting기법

# broadcasting

# 1.  
np.array([[1,2],[3,4]]) + 1

# 2. shape에 1만 있을경우. 크기 보정
np.array([[1,2],[3,4]]) + np.array([[4,5]])

# 3. 2와는 반대로
np.array([[1,2],[3,4]]) + np.array([[10],[10]])

# example
iris['petal_length'] + 10 # broadcasting
-------------------------------------------------------------
수동으로 맞추는방법 - 

1. reshape - 원본미접근, 모양기준, 2차로 변경되기도함.

# 수동으로 맞춰주는법
a = np.arange(10)
a = a.reshape(2, -1) # 모든 음수값은 가능하나 관례상 -1

2. resize - 원본접근, 갯수기준

3. ravel - view

4. flatten - copy
  
5. squeeze - 차원 축소

6. expand_dims - 차원 확대, 차원확장

7. newaxis - 차원확장, 차원 확대
-------------------------------------------------------------
strides를 변경해도 가능
-------------------------------------------------------------
pickle
직렬화 기법
일반 텍스트가 아닌 데이터형을 유지한채 저장하는 방식
-------------------------------------------------------------
r_ : !callable
ix_ : callable
-------------------------------------------------------------
pandas 활용법 두가지
	- 1. EDA (Exploratory Data Analysis) 탐색적 데이터 분석
	- 2. 기계학습을 위한 전처리


-------------------------------------------------------------
numpyFormat
	- 1. pd.DataFrame(numpyFormat) # convert dataFrame
	- 2. pd.read_???
-------------------------------------------------------------
dataFrame 2열이상
	- 인덱스와 헤더가 존재하는 
	- .values하면 numpy포맷으로 변경
	- 2차원	
-------------------------------------------------------------
series 1열
	- .values하면 numpy포맷으로 변경
	- 1차원
-------------------------------------------------------------

열로 추출 - dictionary방식

b['사고년도']
-------------------------------------------------------------
.으로 접근.
dictionary방식 말고 (=data['column']) .방식도있음.
단, 특문이나 numpy의 키워드랑 겹치면안된다.
-------------------------------------------------------------
 (numpy syntax)
b.describe() # summary
b.describe().T #  열과 행 바꾸기 .T

-------------------------------------------------------------
데이터프레임 가져오면 하는것
1. .info()
2. describe() - include값이 기본적으로 숫자만 되어있으므로
-------------------------------------------------------------
loc는 이름으로
iloc는 순수 인덱스번호로
-------------------------------------------------------------
pancy indexing
	iris[iris.columns[:-1]] # pancy indexing
-------------------------------------------------------------
inplace - 그대로 적용을 시킬지의 여부(원본수정)
-------------------------------------------------------------
pandas에서는 index가 X축
-------------------------------------------------------------
# 컬럼추출방법 3가지

	# 1. series
	tips['tip'] 	

	# 2. dataFrame
	tips[['tip']] 

	# 3. 프로퍼티로접근
	tips.tip 
-------------------------------------------------------------
unique
	tips['day'].unique() # 서로다른 갯수
-------------------------------------------------------------
predicate함수

	boolean반환하는 함수
	bool이 왜 True|False 인데, 숫자 1과 0으로 취급되는가 ?
	상속받았는지 여부 확인하기.로 확인가능
	issubclass(bool, int)
-------------------------------------------------------------
시각화해서 보기 ( info, describe 보완 ) 

	mpg.boxplot()

-------------------------------------------------------------
상관계수 
data.corr()

# 색이 진하면 진할수록 음의 상관관계, 색이 옅을수록 양의 상관관계
sns.heatmap(mpg.corr()) # 색분포도로 데이터간 상관관계를 visualization화하여 보는 법
-------------------------------------------------------------
when import data

	1. head - 머리에서 데이터 추출

	2. tail - 바닥에서 데이터 추출

	3. sample - n개만큼 랜덤한 데이터 추출
-------------------------------------------------------------
graph그리기 
대상데이터.plot.원하는그래프형태()
-------------------------------------------------------------
기본적으로 세팅하는 라이브러리

	import numpy as np
	import matplotlib.pyplot as plt
	import pandas as pd
	import seaborn as sns
-------------------------------------------------------------
스타일을 바꾸게되면
	pandas가 plt에 의존하고있어서 pandas도 같이 바뀜
		plt.style.available # 사용가능한 스타일 목록조회
		plt.style.use('ggplot') # 사용하고싶은 스타일 세팅


	history
		from sadf import * # 편하긴하나 무겁.
		import matplotlib.pylab as plt # pylab이 가벼우나 공식홈페이지에서도 지양하는 방법
		import matplotlib.pyplot as plt # 결국 이걸로 사용
-------------------------------------------------------------
그래프수정하기 - State Machine기법 개념 이해해야함.

	plt.figure(figsize=(10, 5))
	plt.axes()
	plt.hist([1,2,1,2,3,4])
	plt.title('Title'); # 제목
	plt.xticks([1,2,3]); # x축 
	plt.yticks([1,3,5,7]); # y축
	plt.xlim([1,2]) # x범위 지정
	plt.ylim([1,2]) # y범위 지정
	plt.xlabel('xTicks Name') # x축 이름
	plt.ylabel('yTicks Name') # y축 이름

-------------------------------------------------------------

# 한글적용하기
	import matplotlib

	폰트리스트 확인
		fm = matplotlib.font_manager.FontManager()
		# fm.ttflist # 현재 컴퓨터에서 사용가능한 폰트리스트 확인
		# <Font 'Noto Sans CJK JP' (NotoSansCJK-Thin.ttc) normal normal 400 normal>

	폰트 적용
		plt.rcParams['font.family'] = 'Noto Sans CJK JP' # 한글 지원하는 폰트로 설정
	
-------------------------------------------------------------
%lsmagic # jupyter에서 지원하는 매직펑션 확인

%matplotlib notebook # 팝업처럼나옴, 물론 현재 os설정에 따라 달라지기도함.
%matplotlib inline # 기본값(=노잼)
-------------------------------------------------------------
pd.read_csv
	header : 헤더 여부

	sep : 구분자
-------------------------------------------------------------
pandas 컬럼 지우기

drop

# data.drop(1, axis=1, inplace=True)
-------------------------------------------------------------
encoding Error 대응

	 Error tokenizing data. C error: Expected 1 fields in line 12, saw 3

	 engine='python' ||
	 sep='|' ||
	 encoding= isEng & 'latin1' || isKo & 'cp949'
-------------------------------------------------------------
pandas 필수 세가지

	aggregation

		groupby
			1. split
				특정 컬럼으로 쪼갬
				
			2. apply
				.mean
				.count
				.max
				.min

			3. combine
				2를 다시 합쳐줌
		pivot
			합쳐서 표현하는게 아니라 단건단건 점찍어서 표현?
		crosstab
-------------------------------------------------------------
!tidy(=wide) -> melt() -> tidy

id_vars : 변경하지 않을 값
-------------------------------------------------------------
vars : instance가 가진 값
-------------------------------------------------------------
데이터 분석 기본 프로세스

	1. import data
	2. isTidy(data) ? 
-------------------------------------------------------------
컬럼명 변경
-------------------------------------------------------------
인덱스만 추출
.index
컬럼만 추출
.column
-------------------------------------------------------------
NaN값 지우기 - missingData

dropna() 
-------------------------------------------------------------
합치기
	stack
	concat
		pd.concat([a,c], axis=1) # a와 c를 concatnate
-------------------------------------------------------------
default setting

import folium
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

plt.style.use('ggplot')
plt.rcParams['font.family'] = 'Noto Sans CJK JP'

-------------------------------------------------------------
pandas.wide_to_long(data)
melt나오기전에 유일한 방법인데, 쓰기 어려움..., 형태를 완벽하게 맞춰야하는 번거로움이 존재.
범용적으로는 melt를 이용하지만, 알고는 있으셈! 
-------------------------------------------------------------
reset_index
	인덱스 번호 변경
	
-------------------------------------------------------------
jupyter notebook theme
pip install jupyterthemes

available list	
	jt - l

set theme
	jt -t chesterish -T -T -cellw 98%
-------------------------------------------------------------
지도 벡터 표현방식 두가지
in Web
topo_json geo_json
-------------------------------------------------------------
from pprint import pprint
pretty 
-------------------------------------------------------------
import time
	시각

현재시각
timestamp :from 1970-01-01

현재로컬시각
localTime = time.localtime()

	# 달로 접근`하기
	localTime.tm_mon # dic
	localTime[1]` # list
-------------------------------------------------------------
import calendar
	달력 찍어주는애
-------------------------------------------------------------
import datetime
	

	import datetime

	x = datetime.date(2019, 11, 27)

	x.year # o
	# x['year']  x
	# x[0] x
-------------------------------------------------------------
인덱스 2개

new_dat스.set_index(['Date', 'forecast']) # 동시에 2개 인덱스

-------------------------------------------------------------
stack (melt와 비슷, melt가 좀더 간편)
	컬럼에 있는 값을 인덱스로 변경
-------------------------------------------------------------
stack과 unstack을 이용해서 원하는대로 구조변경가능

-------------------------------------------------------------
duplicated
	중복체크
	반대는 dropduplicated?
-------------------------------------------------------------
귀여운 그래프스타일
1 plt.xkcd()
2 plt.style.use('ggplot')

plt.rcdefaults() # 초기화

한번만 귀여운스타일
with plt.xkcd():
    tips[['tip']].plot.bar()
-------------------------------------------------------------
1. read_csv
2. tidy_data
	melt
	set_index, stack
	wide_to_long
3. info
	data-type
	head
	miss

4.
5.d

-------------------------------------------------------------
df['column'].value_counts()
	해당 컬럼의 값 갯수 # df(DataFrame)의 column이란 컬럼의 값 갯수
-------------------------------------------------------------
DataFrame에서 
	map
		- only series
		t['petal length (cm)'] = t['petal length (cm)'].map(lambda x:x+1) # map series에서만 사용 가능
	apply
		- series & DataFrame
		- axis, reduce도 포함되어있음.
		t['petal length (cm)'] = t['petal length (cm)'].apply(lambda x:x+1) # apply series&DataFrame 사용 가능
	applymap
		- only DataFrame
-------------------------------------------------------------
데이터범위 지정하고싶을때 사용
	a = np.array([1,2,3,4,5,6,7])
	np.clip(a, 2,5)  # 2 if 2>x elif 5<x 5, 2보다 작으면 2 5보다 크면 5(=fix range(2to5))
	# array([2, 2, 3, 4, 5, 5, 5])
-------------------------------------------------------------
비정형 데이터에서 가장 성능이 좋음.
굳이 인공지능이 아니더라도 똑같이 꾸릴 수있으나,,
가격과 성능을 비교하여 인공지능을 대입할지 

Data로부터 Specific 문제 해결을 위한 최적의 모델 만들기.
Data(비정형데이터 ex: 자연어, 이미지, 영상...)
Specific(매우 협소한 특정 기능, 특정분야 한가지만 가능. 여러가지 동시에 하지못함. 기존에 존재하던 기술을 기반으로 ... 한두가지정도)

기계학습 : 정형데이터가 성능이 좋음.
딥러닝 : 정형데이터도 활용가능하긴함... 그러나...비정형데이터가 성능이 좋음
-------------------------------------------------------------
인공지능의 시작은 
데이터 ==> pandas

가용한 데이터인지 알수 있으려면, 
	1. EDA(비쥬얼라이제이션을 통해 데이터에 대한 빠른 분석) Exploratory Data Analysis
	2. 데이터 전처리(정제) 
그리고 쉽게 정제하기위함. ==> pandas

기계학습의 성능: 알고리즘 < 데이터
-------------------------------------------------------------
인공지능 : 데이터 분석 = 기존 IT : 요구사항 분석

-------------------------------------------------------------
seaborn = 간편하게 그래프출력 + 연습용 데이터
import seaborn as sns # 통계적 그래프를 간단하게 그려줌.
-------------------------------------------------------------
from sklearn.datasets import 
	1. load 작은데이터
	2. fetch 웹데이터
	3. make 랜덤데이터
-------------------------------------------------------------
컬럼 feature || dimension || x값

-------------------------------------------------------------
리그레이션
	실수면 data.info로 확인
클래스피케이션?
-------------------------------------------------------------
기계학습은 반드시 숫자여야만함. (= 인코딩)
통계에 의존성이 큼. (성능과 연관 깊음.)
-------------------------------------------------------------
sns.pairplot = 그려놓고 시작

	sns.pairplot(iris.iloc[:, :-1]) # data는 pandas여야함! 4 x 4 = 16, 16개 출력
		# 그래프 그리기 1. 
	sns.pairplot(iris, vars=iris.columns[:-1], hue='target') # data는 pandas여야함! 4 x 4 = 16, 16개 출력
		# 그래프 그리기 2. hue는 색깔, vars는 사용할 데이터 (= hue에는 'target'은 써야하지만, 그래프에 출력시키면 안되므로..)

-------------------------------------------------------------
ready 4 edu
	x_train, x_test, y_train, y_test = train_test_split(iris.iloc[:,:-1], iris.target)  # x = 0~4줄, y = 1줄, target, label, class값

-------------------------------------------------------------
Features(attributes, dimensions)


classifier	- 영역
	(개,고양이) , (붓꽃종류) 구분
	supervised(분류)
	unsupervised(군집화)

regression 
	나이 숫자 
	선형 회귀(線型回歸, 영어: linear regression)는 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법

쉬운건 영역이 더쉬움
-------------------------------------------------------------
관점별 중요시하는 것
	
	통계 - 해석
	인공지능 - 성능
-------------------------------------------------------------
no free lunch

머신러닝에서는 절대 완벽한 알고리즘은 존재 할 수 없으니 꾸준히 노력하고 여러 분야에 대해 이론을 만들어야 한다는 얘기

why? reason best (=no free lunch)

 ==> 해봐야암 just try

-------------------------------------------------------------
차원의저주


	데이터의 차원이 증가할 수록 (모수가 늘어나므로 ===> 데이터의 개수가 기하급수적으로 늘어나는 문제)

	저차원에서는 충분했던 데이터 양도 고차원이 되면 공간(학습 모델)을 설명하기에 부족해질 수 있음.

	데이터가 작

극복하는법
	
	쓰레기데이터를 수정하거나 제거

	info
		개수가 몇개없으면 모델을 설명하기에 데이터가 부족
-------------------------------------------------------------
data > algo
-------------------------------------------------------------
기계학습은 한계가 있음.
그러나 딥러닝은 데이터와 계속 비례
-------------------------------------------------------------
정형데이터의 경우 성능이
딥러닝 < 기계학습
인 경우도 많음.
-------------------------------------------------------------
놀이터
	http://playground.tensorflow.org
	hidden layer & 알고리즘 & 노드 세팅
-------------------------------------------------------------
블랙박스
	end to end로 결과로 해결된 것을 확인했으나, 과정과정마다의 원리를 아직까지 사람을 이해하지 못한 것.
	설명이 필요한 분야에서는 해당 이슈때문에 사용하지못함 (ex: 의학, ...)
	설명 필요없고 성능만 나와도 사용하는 경향으로 기울어지고있음...
-------------------------------------------------------------
기계학습
	양적데이터 > 질적데이터
-------------------------------------------------------------
쓸모없는 데이터는 필요없지만,

쓸모없는 데이터라고해서 그냥 버릴까? 
no ->
-------------------------------------------------------------
Data wrangling || Data muning

	복잡하고 지저분한 상태의 데이터를 간단한 분석과 접근을 위해 통합하는 과정
-------------------------------------------------------------
모델은 이미 많이 만들어져있다.
승패는 데이터 전처리를 어떻게 하느냐에 따라...
-------------------------------------------------------------
데이터

	정형
		관계형 DB
		컬럼마다 의미가 있음.
		컬럼위치를 변경해도 문제가되지않음.

	반정형

	비정형
		컬럼위치를 변경하면 문제가됨.(ex:image)
		컬럼에 의미가 없음.
		순서가 의미
-------------------------------------------------------------
기계학습 데이터세팅

	정형 -> series

	비정형 -> 1차원
		성능에서 딥러닝알고리즘이 최적
		음성, 자연어, 영상 모두 비정형이므로 딥러닝에서 성능이 가장 잘나옴.
-------------------------------------------------------------
image 전처리하기
	0~255 => 0~1
-------------------------------------------------------------
문자를 숫자로 바꾸는 방법 2가지

	1. label encoding

		- pandas
			# lambda
			tips.sex.map(lambda x: 1 if x == 'Female' else 2)
			# dictionary
			tips.sex.map({'Female':0, 'Male':1})

			# 요일을 숫자로바꾸기

			# dictionary
			dayEncoding = {'Sat':0,'Sun':1,'Thur':2,'Fri':3}
			tips.day.map(dayEncoding)
			# lambda
			#tips.day.map(lambda x: 1 if x == 'Female' else 2)

			pass

		- sklearn

			from sklearn.preprocessing import ???

			# le.fit(mpg.origin)
			# le.transform(mpg.origin)

			# 한방
			mpg.origin = le.fit_transform(mpg.origin)
			# mpg.origin.map({'usa':0, 'europe':1, 'japan': 2}) # pandas로 하게되면 되돌릴수 없으나, 장단점이있음.		


	2. one hot encoding
		
		- pandas
			
			pd.get_dummies(mpg.origin)

		- sklean

			from sklearn.preprocessing import OneHotEncoder

			ohe = OneHotEncoder()

			# ohe.fit_transform(mpg[['origin']]) 해서

			#<398x3 sparse matrix of type '<class 'numpy.float64'>'
				#with 398 stored elements in Compressed Sparse Row format>
					#가 나오면 toarray        
					
			ohe.fit_transform(mpg[['origin']]).toarray()
			# ohe.inverse_transform([[0,0,1]])
-------------------------------------------------------------
기계 학습 준비물 만들기

data.target_names
a = pd.DataFrame(data.data, columns=data.feature_names) # DataFrame Instance
b = pd.DataFrame(data.target, columns=['target'])
iris = pd.concat([a,b],axis=1) # data ready done
-------------------------------------------------------------
hold out - train_test_split
	가장 좋은 알고리즘을 찾기위한 
	데이터의 공정성을 최대화.
		no free lunch(데이터에 따라 성능차이가 있기때문에)


crossValidation
		데이터가 너무 적을경우,, 하나만 틀려도 성능차이에 큰 요인으로 다가오므로,,,
		모든데이터(트레인, 테스트데이터)가 평균적으로 들어가므로...
		데이터를 나눌때 고려할 필요없이 편리하게 사용 가능
-------------------------------------------------------------
train_test_split

	random_state

	stratify ()
		클래스피케이션일때만가능
		y값(target) 기준으로 비율을 맞춰서 잘라주는 옵션
			데이터가 적을때 사용하면 좋음.
				데이터가 많으면 큰수의법칙에 의해 굳이 필요없는 옵션이지만, 
				그렇지 않을 경우에는 해당 옵션을사용하여 큰수의법칙 부정적인 영향을 방어

	n_jobs
		-1 : 프로세서 풀가동

	
-------------------------------------------------------------
다중 공선성
-------------------------------------------------------------
큰 수의 법칙

	작은 데이터에서는 균일하지 못하게 나올 확률이 높지만,
	데이터의 양이 많으면 많을수록 평균적인 분포와 가까워진다는 법칙
	ex) 주사위

-------------------------------------------------------------
2종 오류
	2ne

-------------------------------------------------------------
cross_val_score
	데이터가 적을때 반드시 사용할 것.
	같은 데이터를 여러 방면으로 시도하기때문에 더 많은 데이터를 사용하는 것과 비슷한 효과를 가지게된다.(=데이터확보)
	+ hyper parameter 찾을때 사용 (hyper parameter : 사람이 찾아줘야하는 데이터)

	- 이용못하는 경우 : 최종결과물에만 크로스벨리데이션(crossValidation)기법을 사용하지않음.

	from sklearn.model_selection import cross_val_score

	from sklearn.neighbors import KNeighborsClassifier
	from sklearn.ensemble import RandomForestClassifier

	cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:, -1], cv = 10)


		estimator
			알고리즘을 인스턴스
		
-------------------------------------------------------------
sklearn에서 기본적으로 숫자데이터만 처리가가능하지만,

Y값을 문자로 썼을때는 숫자로 바꿔줌...
-------------------------------------------------------------
pip install pandas-profiling

import pandas_profiling as pp

ProfileReport
-------------------------------------------------------------
모델이 복잡할수록 성능이 좋음.
 + 차원이 증가 === 데이터가 많이필요.
-------------------------------------------------------------
Occam's Razor 오캄의 면도날

	동가홍상
		비슷한 성능이면 더심플한 것을 선택
			=== 간단한 모델을 선택
				=== 데이터양 더 적게, 
-------------------------------------------------------------
편향과 편차를 잘 조절해야함.

편향과 편차의 관계

bias(편향)/variance(편차)

	overfitting (기계학습에서 )
		- 지나치게 값에 치우쳐짐. (학습데이터에서는 좋지만, 실제 예측할때는 성능이 좋지않음.)
		- 편차가 큰 모델
		- 모델이 복잡해질수록 오버피팅 확률높아짐

	underfitting
		- 지나치게 몰려져있음
		- 편향이 큰 모델
		- 일어나는 확률이 높음
	
-------------------------------------------------------------
모델의 고려사항

	성능
	편차
-------------------------------------------------------------
cv를 했을 경우, 데이터 관리

from sklearn.model_selection import KFold # 데이터관리할때 편리함. 어떤 데이터를 사용했는지.

kf = KFold(7, shuffle=True)
cross_val_score(KNeighborsClassifier(), wine.iloc[:,:-1], wine.target, cv = kf)
for trainData, testData in kf.split(wine.iloc[:,:-1], wine.target):
    print(trainData) # 트레인데이터
    print(testData) # 테스트데이터
-------------------------------------------------------------
train_score train_data로 테스트한 데이터 
test_score test_data로 테스트한 데이터?

-------------------------------------------------------------
# cv에 사용할수 있는 애들.

from sklearn.model_selection import StratifiedKFold # class비율까지 고려한 KFold (stratify를 고려) === 더 정확한 테스트 가능 == 
from sklearn.model_selection import ShuffleSplit # 셔플
from sklearn.model_selection import StratifiedShuffleSplit # class비율 고려하고 셔플
-------------------------------------------------------------
표준편차 기법 (표준화)

	편차를 줄여서 영향을작게
	ex) mean() 한값이 크게나오는 경우 표준편차를 사용

-------------------------------------------------------------
Perceptron 딥러닝과 관련된 알고리즘
-------------------------------------------------------------
in sklearn

	learningCurve
		러닝커브가 더 중요함.

	validationCurve
		gridSearch 방식으로 대체가능(타 프레임워크에서는 없는 경우도 있으므로 validationCurve도 사용할 수 있어야함.)
-------------------------------------------------------------
	statistical learningCurve
		설명이 가능해야함 => 간단 => 차원축소

	machine learningCurve

		
-------------------------------------------------------------

feature_selection ( 성능 문제 때문에 )
-------------------------------------------------------------
	chi - squared
	카이 스퀘어?

	SelectKBest 

		# 오버피팅 방어
		# 가장 좋은 n개 선택하는 법 - 10개 데이터중에 가장 중요한 n개만 선택
		from sklearn.feature_selection import chi2, SelectKBest 

	

-------------------------------------------------------------
sklearn

	datasets
		3
	
	model_selection

	preprocessing

	feature_selection

	metrics

-------------------------------------------------------------
TP : true - true
TN : false - false

precision C = 
-------------------------------------------------------------
LogisticRegression
	타 프레임워크에서는 3개이상 구분을하지못함. linear모델이기 때문에 두가지만 구분가능.
	하지만 sklearn에서는 내부적으로 처리해줌.
-------------------------------------------------------------
레이어가 많으면 성능이 좋아짐.
	대신 그리고 미분을 계속하다가 0이되버리는 순간에 업데이트가 안될수도있고,
	오버피팅되는 문제와 학습안되는 문제를 해결하지못했었음.
	하지만 해결한게 딥러닝


	그럼에도 복잡하면 복잡할수록 오버피팅이 생김.
	
		
-------------------------------------------------------------

집단지성

	VotingClassifier

	제일 좋은 모델찾기위해서 서로 대결 시키는건데, 리소스가 엄청나게 많이들어감.

	합치면 강해지는 경우, 
	서로 싸워서 더 높은 결과를 반환
	=> 예측모델만들 때, 보통 마무리단계에서 앙상블을 사용함.
	current 60 , need 80 ,if use ensemble, possible over 80
	weights 가중치
	
	BaggingClassifier
		오버피팅이 잘나는 모델에 대해서 
		학습데이터에 너무 치우쳐서 학습된 모델일때,

	boosting
		오버피팅 잘나지만, 성능 좋음. but 시간이오래걸림
		--> 오버피팅 잘나지만 성능을 우선 올리고, 오버피팅을 보완하는 식으로 가는것이 현 메타?

		임계치를 정해두고, 임계치에 올라올때까지 교육시키고, 넘어가면 꺼내옴.

		AdaBoostClassifier
			
			base_estimator 교육시킬 모델

		gradient_boosting

		xdboosting?
			현재 가장 많으쓰인다고함.

		stackingClassifier
			학습된 결과가 모델로 됨.
			속도는 진짜 느리지만, 성능에서 최고.

-------------------------------------------------------------
mlxtend
	
-------------------------------------------------------------
deep learnigng

	numpy 
-------------------------------------------------------------
# 첫번째 이미지 추출하기
# X_train.shape

# X_train[0, :, :]
# X_train[0,...]
X_train[0]
-------------------------------------------------------------
학습시킬때는 반드시
1차원으로 만들어주어야함.
예를들어 이미지 2차원짜리도 1차원으로 쭉 늘여야만 학습이 가능한 데이터가 된다.
ex) numpy포맷일 경우, reshape이나 flatten으로 ...
-------------------------------------------------------------
레이어가 간단할경우.
직선이고, 
그런 레이어가 중첩되면 될수록,
곡선을 표현할수있게된다.
-------------------------------------------------------------
deepLearning base of base 
lowest 

- AND gate

	w1x1 + w2x2 + b

	x1 x2
	0  0 => 0 : b < 0
	0  1 => 0 : w2 + b < 0
	1  0 => 0 : w1 + b < 0
	1  1 => 1 : w1 + w2 + b < 0

- OR gate

AND와 OR는 직선으로 표현할 수 있기때문에
classification

	x1 x2
	0  0 => 0 : b < 0 
	0  1 => 1 : w2 + b > 0
	1  0 => 1 : w1 + b > 0
	1  1 => 1 : w1 + w2 + b > 0

	b = -0.1
	w2 > 0.1 -> w2 = 0.2
	w1 > 0.1 -> w1 = 0.2

	0.2 + 0.2 + -0.1 > 0 



- XOR gate

퍼셉트론은 XOR를 못풀었다. 근데 무슨 인공지능이냐??라고
공격을 당했으나, 선 2개를 이용해서 결국 품. 
=> 중첩을 하게되면 직선뿐만 아니라 곡선으로 표현할 수 있고
=> 그말은 더 복잡한 것도 표현이 가능해짐.
=> 레이어가 많으면 많을수록 복잡한 것도 찾을 수가 있음.
=> none LinearModel

	x1 x2
	0  0 => 0 : b < 0 
	0  1 => 1 : w2 + b > 0
	1  0 => 1 : w1 + b > 0
	1  1 => 0 : w1 + w2 + b < 0
	

-------------------------------------------------------------
Sequencial Model
	레이어들의 진행방향이 순차적인 모델
-------------------------------------------------------------
입력이 몇개 들어올지 모르니
첫번째 레이어에는 반드시 입력값의 모양을 설정해주어야한다.
+ 기본적으로 1차원으로 변경해서 설정해야한다. 
-------------------------------------------------------------
deepLearning의 파라미터 수는
모든 입력값에 바이어스갯수
ex ) 28*28
레이어2개면
	(입력모양))*(레이어개수)) + (바이어스개수)
	(28*28)*(2) + 2
	28*28*2+2 
	=> 1570
-------------------------------------------------------------
activation
	다음 신경망으로 전달하는 방식
	보통 'relu'

	softmax - 랜덤으로 (확률)

마지막 레이어(output layer)
	activation 
-------------------------------------------------------------
Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs,
)
-------------------------------------------------------------
model.compile(
    optimizer='rmsprop',	# 미분방식
    loss=None,				# 최소화
    metrics=None,
    loss_weights=None,
    sample_weight_mode=None,
    weighted_metrics=None,
    target_tensors=None,
    distribute=None,
    **kwargs,
)
-------------------------------------------------------------
model.fit(
    x=None,					# x_train
    y=None,					# y_train
    batch_size=None,
    epochs=1,				# 학습횟수
    verbose=1,
    callbacks=None,
    validation_split=0.0,
    validation_data=None,
    shuffle=True,
    class_weight=None,
    sample_weight=None,
    initial_epoch=0,
    steps_per_epoch=None,
    validation_steps=None,
    validation_freq=1,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
    **kwargs,
)
-------------------------------------------------------------
sklearn
MLPClassifier(
    hidden_layer_sizes=(100,),		# mlp = MLPClassifier((3,2,4)) ==> 첫번째는 3개, 두번째는 2개, 세번째는 4개
    activation='relu',
    solver='adam',		# 기계학습의 목적: 실제값과 예측값이 최소화하는 것 ==> 미분 방식
    alpha=0.0001,
    batch_size='auto',
    learning_rate='constant',
    learning_rate_init=0.001,
    power_t=0.5,
    max_iter=200,
    shuffle=True,
    random_state=None,
    tol=0.0001,
    verbose=False,
    warm_start=False,
    momentum=0.9,
    nesterovs_momentum=True,
    early_stopping=False,
    validation_fraction=0.1,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-08,
    n_iter_no_change=10,
)
-------------------------------------------------------------
기계학습
1. model
	- Sequencial
	- Model
2. layer
	첫번째 input_shape
	마지막 목적
		loss function
		crossentropy (=classification문제일때 차이를 나타내주는 함수 - 
			1. 두개면 binaryclassentropy 
			2. 여러개면 categorycalcrossentropy
		)
		class [multi, bi]
		rense

		sparse
			1. 원핫인코딩안되있을때,
3. compile

optimizer loss function을 최적화시키는 방법
	1.
	2.
	1과 2가 있으나 1과 2 둘다 갖고있는게 adam이므로 보편적으로 adam을 사용

	추후에는 보편적인 adam을 사용해보고, 조금씩 수정하면서 최적화를 시키는 방향으로
-------------------------------------------------------------
predict 예측
행렬 곱
w와 b를 찾는것이 학습

epochs 전체데이터를 보고나서 업데이트
	성능좋으나, overfitting일어날 확률 높음.

batch_size batch_size만큼만 보고나서 업데이트하지않고, 
	너무 작게 하게되면, 잘못된 학습만 보게될 수도있고,,
-------------------------------------------------------------
back propagation 
	뒤에서 앞으로 찾는방식?
	gradient 

-------------------------------------------------------------
다른 모델 가져다쓰기.

1. tensorflow hub는 이미 만들어진 모델을 갖다쓰는것.
pip install tensorflow-hub || pip install tf-hub
# 버전차이로 명칭이 다를수도있음.
사용하기 편한대신 커스터마이징불가능.


-------------------------------------------------------------
Convolutional Neural Network 합성곱 신경망

고유의 특성뽑기
특징찾기

Fully Connected Layer1 만으로 구성된 인공 신경망의 입력 데이터는 1차원(배열) 형태로 한정.
한 장의 컬러 사진은 3차원 데이터. 
배치 모드에 사용되는 여러장의 사진은 4차원 데이터.
사진 데이터로 전연결(FC, Fully Connected) 신경망을 학습시켜야 할 경우에, 
3차원 사진 데이터를 1차원으로 평면화시켜야 함.
그러다보니 사진 데이터를 평면화 시키는 과정에서 공간 정보가 손실될 수밖에 없음.
결과적으로 
	이미지 공간 정보 유실로 인한 정보 부족으로 인공 신경망이 특징을 추출 및 학습이 비효율적이고 정확도를 높이는데 한계가 있습니다. 
	이미지의 공간 정보를 유지한 상태로 학습이 가능한 모델이 바로 CNN(Convolutional Neural Network)입니다.

Fully Connected

예측하는것. 예측
-------------------------------------------------------------
transfer learning
기존의 만들어진 모델을 사용하여 새로운 모델을 만들시 학습을 빠르게 하며, 예측을 더 높이는 방법

왜 사용하나?

    실질적으로 Convolution network을 처음부터 학습시키는 일은 많지 않음. 
		대부분의 문제는 이미 학습된 모델을 사용해서 문제를 해결할 수 있습니다.
    복잡한 모델일수록 학습시키기 어렵습니다. 
		어떤 모델은 2주정도 걸릴수 있으며, 비싼 GPU 여러대를 사용하기도 합니다.
    layers의 갯수, activation, hyper parameters등등 고려해야 할 사항들이 많으며, 
		실질적으로 처음부터 학습시키려면 많은 시도가 필요합니다.

=> 결론적으로 이미 잘 훈련된 모델이 있고, 특히 해당 모델과 유사한 문제를 해결시 transfer learining을 사용

여기에 커스터마이징을 하여 사용 == transferLearning

그러려면 기본적으로 커스터마이징하려면

	1. 레이어 개수확인하기.
		len(model.layers)
	2. 레이어 제거하기.
		mo = Sequential(mobile1.layers[:-3]) # 뒤의 세개 제거
	3. 레이어 추가하기.
		mo.add()
가 선행되어야한다.

그리고 이미 갖고있는 학습가능한 것들은 재학습시킬필요없음.
==> 추가로 학습할 데이터만 학습시키기위해 이미 갖고있는 학습가능한 파람을 제거

-------------------------------------------------------------
keras를 통해 임의의 이미지 불러오기

import numpy as np
import PIL.Image as Image

# tf.keras.utils.get_file(${파일명}, ${url})

# 이미지불러오는방법 1
grace_hopper = tf.keras.utils.get_file('%ED%94%BC%EC%B9%B4%EC%B8%84.jpg','https://post-phinf.pstatic.net/MjAxNzEyMTNfMjIx/MDAxNTEzMTI1MDYwNTUw.Q6LO7KVPRttYT6Y8eSCkMUSjxvXW9IWMSVCQmWVwOwAg.g6zfoPUlOJfSXTcja4bKgesJyYakdIEehHTAW1rrnogg.JPEG/%ED%94%BC%EC%B9%B4%EC%B8%84.jpg?type=w1200')
grace_hopper = Image.open(grace_hopper).resize(IMAGE_SHAPE)
grace_hopper

# 이미지불러오는방법 2
x = tf.keras.preprocessing.image.load_img('/home/ai39/jupyter/1219/chu.jpg', target_size=(224,224))
-------------------------------------------------------------
# image convert to numpyFormat

# 이미지포맷을 numpy포맷으로 변경하기 1
t = tf.keras.preprocessing.image.img_to_array(x)

# 이미지포맷을 numpy포맷으로 변경하기 2 
s = np.array(x)

# 이미지포맷을 numpy포맷으로 변경하기 3


-------------------------------------------------------------
import numpy as np
np.argmax(s)

입력값중에서 가장 큰값의 인덱스반환하는 함수
input : numpyFormat
output : the largest of input's index
-------------------------------------------------------------
# 새로운 이미지불러와서 학습결과 확인하기

# 1. load image
image2 = tf.keras.preprocessing.image.load_img('/home/ai39/jupyter/1219/pocketmon.gif', target_size=(224,224))

# 2. convert format, imageFormat to numpyFormat
t = tf.keras.preprocessing.image.img_to_array(image2)

# 3. sync Format by newAxis
trainData = np.array(t)

# 4 . predict
model.predict(trainData)
-------------------------------------------------------------
레이어 개수 확인
len(model.layers)
-------------------------------------------------------------
설치는 - pip install tensorflow-datasets
사용은 _ import tensorflow_datasets
-------------------------------------------------------------
연습용 데이터 활용하기
import tensorflow_datasets as tfds

-------------------------------------------------------------
multi input multi output일 경우,
Sequencial사용하지못함.

multi input multi output 확인법
summary()에서 우측에 Connected to가 있음?
Model: "mobilenetv2_1.00_224"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
-------------------------------------------------------------
합성함수가 곧 인풋넣고 액티베이션통과하고 다음넘어가는 과정과 평행
==> functional

이론과 코딩이 비슷해지는 것으로 인한 장점
형식적 증명 가능성
이론적인 장점은 함수형 프로그램이 정확하다는 수학적 증명을 만드는 것이 더 쉽다.

==> functional API
사용할 수 있는 문법적 이유 : __call__로 인해 인스턴스와 동시에 기존에 모델세팅하던 행위를 하면서 다음레이어로 바로 진행가능.

# 전문가용을 하기위한 개념 __call__ 이란? instance도 동일하게 된것

1. 문법적 확인
class B:
    def __call__(self):
        print('aaa')

x = B()
# result : 'aaa'
x = B()()
# result : 'aaa'

2. 실제 모델 세팅시에 적용하면?

input = Input(shape=(784,))
layers1 = Dense(64, activation='relu')(inputs)
layers2 = Dense(64, activation='relu')(layers1)
output = Dense(10, activation='softmax')(layers2)

# total =  Dense(10, activation='softmax')(layers2)(layers1)
-------------------------------------------------------------
리스코프의 치환원칙

override할수있는 근거
-------------------------------------------------------------
tensorflow특징 numpy와 다르게

cast
gpu사용가능
immutable


-------------------------------------------------------------
Neural Style transfer

-------------------------------------------------------------
numpy와
1.cpu
2.immutable
3.

tensor의 차이
1.gpu
2.

변환과정시 시간이 오래걸리므로,
처음부터 tensor로 불러오는게 이득.(if tensorflow)

-------------------------------------------------------------
from tensorflow.keras.applications import
-------------------------------------------------------------
from tensorflow.keras.applications import VGG19
vg = VGG19(include_top = False)
인스턴스시
Convolution(특징만 가져오는것)만 하므로 include_top = False
예측까지 할경우 fullyconnect해야하므로 include_top = True

-------------------------------------------------------------
feature extraction 특징 추출

-------------------------------------------------------------
자동미분 (최적의 Weight와 bias를 찾는것)

자동으로 모델만들어주고 weight 업데이트해주는것.

loss functino이 정해지지않았을 경우,
custom loss function(사용자 정의 로스함수)

==> 
로스가 복잡하면 fit을 사용하지 못하므로,
직접 fit을 만들어야함
gradient tape로 사용해야함.


-------------------------------------------------------------